# LLM & Fine-tuning
torch==2.1.2
transformers==4.36.2
peft==0.7.1  # LoRA/QLoRA
bitsandbytes==0.41.3  # Quantization
accelerate==0.25.0
datasets==2.16.1

# Training
trl==0.7.10  # Transformer Reinforcement Learning
wandb==0.16.2  # Experiment tracking (optional)

# Inference
vllm==0.2.7  # Fast inference (optional)
